{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Visualizations and pre-processing\n",
    "Here we'll visualize the data to better explore it and establish the pre-processing steps most reasonable.\n",
    "\n",
    "> **Note**: The graphs render very slow, so the interactive ones are saved to html. If you want to see them, you can open the html files in your browser after running the notebook. The static ones are shown in the notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Index id invalid",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[0;32m----> 3\u001B[0m df_transactions \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtransactions_cleaned.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mid\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m df_transactions\u001B[38;5;241m.\u001B[39minfo()\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.2/envs/house_transactions/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[0;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.2/envs/house_transactions/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[1;32m    326\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    327\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[1;32m    329\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[1;32m    330\u001B[0m     )\n\u001B[0;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.2/envs/house_transactions/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m    936\u001B[0m     dialect,\n\u001B[1;32m    937\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    946\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m    947\u001B[0m )\n\u001B[1;32m    948\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m--> 950\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.2/envs/house_transactions/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    608\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[1;32m    610\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[0;32m--> 611\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.2/envs/house_transactions/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1778\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m   1771\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1773\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[1;32m   1774\u001B[0m     (\n\u001B[1;32m   1775\u001B[0m         index,\n\u001B[1;32m   1776\u001B[0m         columns,\n\u001B[1;32m   1777\u001B[0m         col_dict,\n\u001B[0;32m-> 1778\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[1;32m   1779\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[1;32m   1780\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1781\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m   1782\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.2/envs/house_transactions/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:321\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m    318\u001B[0m     data \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, (i, v) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(names, data_tups)}\n\u001B[1;32m    320\u001B[0m     names, date_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_date_conversions(names, data)\n\u001B[0;32m--> 321\u001B[0m     index, column_names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdate_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malldata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m index, column_names, date_data\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.2/envs/house_transactions/lib/python3.11/site-packages/pandas/io/parsers/base_parser.py:379\u001B[0m, in \u001B[0;36mParserBase._make_index\u001B[0;34m(self, data, alldata, columns, indexnamerow)\u001B[0m\n\u001B[1;32m    376\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    378\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_complex_date_col:\n\u001B[0;32m--> 379\u001B[0m     simple_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_simple_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43malldata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    380\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_agg_index(simple_index)\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_complex_date_col:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.2/envs/house_transactions/lib/python3.11/site-packages/pandas/io/parsers/base_parser.py:411\u001B[0m, in \u001B[0;36mParserBase._get_simple_index\u001B[0;34m(self, data, columns)\u001B[0m\n\u001B[1;32m    409\u001B[0m index \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    410\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex_col:\n\u001B[0;32m--> 411\u001B[0m     i \u001B[38;5;241m=\u001B[39m \u001B[43mix\u001B[49m\u001B[43m(\u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    412\u001B[0m     to_remove\u001B[38;5;241m.\u001B[39mappend(i)\n\u001B[1;32m    413\u001B[0m     index\u001B[38;5;241m.\u001B[39mappend(data[i])\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.2/envs/house_transactions/lib/python3.11/site-packages/pandas/io/parsers/base_parser.py:406\u001B[0m, in \u001B[0;36mParserBase._get_simple_index.<locals>.ix\u001B[0;34m(col)\u001B[0m\n\u001B[1;32m    404\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    405\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m col\n\u001B[0;32m--> 406\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIndex \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcol\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m invalid\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Index id invalid"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib2 import Path\n",
    "df_transactions = pd.read_csv(Path(\"data\") / \"transactions_cleaned.csv\", index_col='id')\n",
    "\n",
    "df_transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic Histograms\n",
    "\n",
    "Let's plot histograms for every column and see if there are any outliers and if the data is balanced."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# split up dataframes by data type (should be all if all types converted correctly above)\n",
    "df_num = df_transactions.select_dtypes(include=np.number)\n",
    "\n",
    "# first let's plot all the numerical columns using a distribution plot\n",
    "for col in df_num:\n",
    "    # fig = ff.create_distplot([df_transactions[col]], group_labels=[col], bin_size = 0.5)\n",
    "    # fig.write_html(str(Path(\"figures\") / f\"{col}_distplot.html\"))\n",
    "    fig = px.histogram(df_transactions[col], nbins=100, marginal='box', title=f\"{col} distribution\")\n",
    "    fig.write_html(str(Path(\"figures\") / f\"{col}_histplot.html\"))\n",
    "    fig.show(renderer='svg')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### outliers\n",
    "Togethe rwith the `df.describe` function we've run in `exploration` seen that some numerical columns have very high max value, but much lower qt 75% value. This suggests outliers and with those distribution plots. Here are the columns with outliers:\n",
    "- price( and totalPrice) have outliers towards higher value (no extreme)\n",
    "- ladderRatio has one very high outlier.\n",
    "- square has outliers going to higher values, with 2 larger ones\n",
    "- constructionTime some outliers towards lower values.\n",
    "- tradetime some outliers towards lower values.\n",
    "- followers some outliers towards higher values.\n",
    "- DOM some outliers towards higher values.\n",
    "\n",
    "### (Im)Balanced columns\n",
    "We have a lot of categorical data. These can be problematic if one category is way more prevalent than the other ones. This can lead to a model that is biased towards the more prevalent category. Here are the columns with a very unbalanced distribution:\n",
    "- renovationCondition doesn't have much data points in 1 (corresponding to 2; Rough condition)\n",
    "- buildingStructure doesn't have much data points in 0 (1:unknown), 2 (3: concrete), 4 (5:steel)\n",
    "- kitchen really only has value in 1 kitchen <-- can be thrown out\n",
    "- All the rooms have some categories with very low amount of data points, but non as extreme (to one category) as kitchen.\n",
    "\n",
    "## Conclusions to deal with outliers and (im)balanced data:\n",
    "1. Delete the few extreme rows for ladderRatio and square\n",
    "2. If applying a scaler, probably want to use a robust scaler to give the outliers in other categories less influence\n",
    "3. Throw out the kitchen column\n",
    "4. For the remaining categorical data, let's do one-hot encoding for the ones that have more than 2 categories.\n",
    "      - `floorPosition`\n",
    "      - `renovationCondition`\n",
    "      - `buildingStructure`\n",
    "      - `district`\n",
    "Note: we don't do this with cId, way to many categories. We might drop this column later anyway."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " current shape (298561, 43)\n",
      "shape after removing outliers and kitchen (297838, 43)\n",
      "shape after encoding: (297838, 43)\n"
     ]
    }
   ],
   "source": [
    "print(\" current shape\", df_transactions.shape)\n",
    "# Filter out the extreme outliers for square and ladderRatio by cutting off the top 0.001% of the data\n",
    "df_transactions = df_transactions[df_transactions['square'] < df_transactions['square'].quantile(0.999)]\n",
    "df_transactions = df_transactions[df_transactions['ladderRatio'] < df_transactions['ladderRatio'].quantile(0.999)]\n",
    "\n",
    "# drop kitchen column\n",
    "try:\n",
    "    df_transactions.drop(columns=['kitchen'], inplace=True)\n",
    "except KeyError:\n",
    "    pass\n",
    "print(\"shape after removing outliers and kitchen\", df_transactions.shape)\n",
    "\n",
    "# One-hot encode the categorical data\n",
    "multi_cat_cols = ['floorPosition', 'renovationCondition', 'buildingStructure', 'district']\n",
    "# get_dummes adds extra columns for each category (n_cat), and we drop the first one to avoid collinearity\n",
    "try:\n",
    "    df_transactions = pd.get_dummies(data=df_transactions, columns=multi_cat_cols, drop_first=True)\n",
    "except KeyError:\n",
    "    pass\n",
    "print(f\"shape after encoding: {df_transactions.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "only lost ~700 rows to the extreme outliers, so that's fine.\n",
    "\n",
    "## Correlation Matrix\n",
    "From the correlation matrix below we can see some positive correlation with price in:\n",
    "- tradeTime\n",
    "- DOM\n",
    "- communityAverage\n",
    "- totalPrice\n",
    "- renovationCondition\n",
    "- followers\n",
    "- subway\n",
    "- Some districts\n",
    "and some highly negative corelation with price in:\n",
    "- Lng\n",
    "- constructionTime\n",
    "- square\n",
    "- Lng\n",
    "- drawingRoom"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Directly copied from: https://www.geeksforgeeks.org/display-the-pandas-dataframe-in-heatmap-style/\n",
    "df_transactions.corr().style.background_gradient(cmap='viridis') \\\n",
    "    .set_properties(**{'font-size': '20px'})\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Heatmap of price\n",
    "We saw some strong negative correlation with lng and some correlations with different districts. Given that in the housing market it's all about location. I am suspecting that the price can be depended on the precise location"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import folium\n",
    "import branca.colormap as cmp\n",
    "\n",
    "# Let's plot the data on a map with price as heatmap color\n",
    "df_heatmap = df_transactions[['Lng', 'Lat', 'price']]\n",
    "\n",
    "# Getting color gradient\n",
    "linear = cmp.LinearColormap(\n",
    "    ['green', 'red'],\n",
    "    vmin=min(df_heatmap['price']), vmax=max(df_heatmap['price']),\n",
    "    caption='price of property'\n",
    ")\n",
    "# Create a map\n",
    "m = folium.Map(location=[40, 116], zoom_start=10)\n",
    "for _, row in df_heatmap.iterrows():\n",
    "    col_grad = linear(row['price'])\n",
    "    folium.CircleMarker(\n",
    "        location=(row['Lat'], row['Lng']),\n",
    "        radius=1,\n",
    "        color=col_grad,\n",
    "        fill=False,\n",
    "        fill_color=col_grad,\n",
    "        fill_opacity=0.3\n",
    "    ).add_to(m)\n",
    "linear.add_to(m)\n",
    "\n",
    "# Can't render the map in notebook statically, so only save.\n",
    "m.save(str(Path(\"figures\") / \"price_map.html\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## City center more expensive.\n",
    "It looks like the city center (of Beijing!) is more expensive, than more to the outside. It looks like somewhat of a circular dependence. Let's keep this in mind for the model later.\n",
    "\n",
    "## Time dependency\n",
    "One of the strongest correlators with price is the time. Let's see how this dependency looks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "df_transactions.sort_values(by='tradeTime', inplace=True)\n",
    "price_rolling_avg = df_transactions['price'].rolling(window=2000, min_periods=50).mean()\n",
    "datetimes = pd.to_datetime(df_transactions['tradeTime'])\n",
    "\n",
    "# Create figure and add lines\n",
    "fig = go.Figure(data=go.Scatter(x=datetimes, y=df_transactions['price'], name='transaction', mode='markers', marker=dict(size=2, opacity=0.2)))\n",
    "fig.add_trace(go.Scatter(x=datetimes, y=price_rolling_avg, name='rolling avg (2000)', mode=\"lines\",\n",
    "                         line=dict(color='red', width=5)))\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"time\",\n",
    "    yaxis_title=\"price\",\n",
    "    title=\"price over time\"\n",
    ")\n",
    "fig.write_html(str(Path(\"figures\") / \"time_v_price.html\"))\n",
    "fig.show(renderer=\"svg\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Price over time\n",
    "The price seems to increase of time with dipping in the end. Consdering we're going to split the data based on time, this might be right around were we'll split the data in training and test set. Since we're using the datetime data to split it between testing and training, and it's not a simple linear increase over time, we probably don't want to use the time data for this model. Let's also drop totalprice, since that's the same as price * square and it wasn't originally in the dataset.\n",
    "\n",
    "## Split data in training and test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop totalprice\n",
    "try:\n",
    "    df_transactions.drop(columns=['totalPrice'], inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Let's add the datetimes column back for splitting the data\n",
    "df_transactions['tradeTime'] = datetimes\n",
    "\n",
    "# now let's split the data in training and test set by making everything before the start of 2017 training data:\n",
    "df_train = df_transactions[df_transactions['tradeTime'] < pd.to_datetime('2017-01-01')]\n",
    "df_test = df_transactions[df_transactions['tradeTime'] >= pd.to_datetime('2017-01-01')]\n",
    "print(f\"training shape: {df_train.shape}\")\n",
    "print(f\"test shape: {df_test.shape}\")\n",
    "print(f\"training set is: {df_train.shape[0] / (df_train.shape[0] + df_test.shape[0]) * 100}% of the data \")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Scaling\n",
    "Now that we've removed extreme outliers, we can scale the data. There are 3 main scaling methods: Standard, Robust and MinMax scaler. Bets are the Robust scaler will work best, as it is less sensitive to outliers. Let's see how the data looks after scaling with all methods to confirm."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
    "\n",
    "# drop the tradeTime column in the data, as we can't scale that and decided not to use it.\n",
    "for df in [df_train, df_test]:\n",
    "    try:\n",
    "        df_train.drop(columns=['tradeTime'], inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Do all the scaling, only with the training data\n",
    "std_scaler = StandardScaler()\n",
    "array_num_standard_scaled = std_scaler.fit_transform(df_train)\n",
    "df_standard_scaled = pd.DataFrame(array_num_standard_scaled, columns=df_train.columns)\n",
    "rbst_scaler = RobustScaler()\n",
    "array_num_robust_scaled = rbst_scaler.fit_transform(df_train)\n",
    "df_robust_scaled = pd.DataFrame(array_num_robust_scaled, columns=df_train.columns)\n",
    "minmax_scaler = MinMaxScaler()\n",
    "array_num_minmax_scaled = minmax_scaler.fit_transform(df_train)\n",
    "df_minmax_scaled = pd.DataFrame(array_num_minmax_scaled, columns=df_train.columns)\n",
    "\n",
    "# Compare the scaling methods\n",
    "scaling = (\"no\", \"standard\", \"robust\", \"minmax\")\n",
    "top_features = {}\n",
    "for i, df in enumerate((df_train, df_standard_scaled, df_robust_scaled, df_minmax_scaled)):\n",
    "    print(f\"Scaling: {scaling[i]}\")\n",
    "    Y = df['price']\n",
    "    X = df.drop(columns=['price'])\n",
    "    pca = PCA(n_components=3).fit(X)\n",
    "    X_pca = pca.transform(X)\n",
    "    print(f\"Explained variance: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"Total explained variance: {np.sum(pca.explained_variance_ratio_)}\")\n",
    "    # initialize a dictionary to store the explained variance for each feature\n",
    "    dict_features_explained = {X.columns[i]: 0 for i in range(len(X.columns))}\n",
    "    for i, col in enumerate(X.columns):\n",
    "        for pca_component in pca.components_:\n",
    "            dict_features_explained[col] += abs(pca_component[i])\n",
    "    dict_features_explained = sorted(sorted(dict_features_explained.items()), key=lambda x: x[1], reverse=True)\n",
    "    top_15_dict = dict_features_explained[:15]\n",
    "    for key, value in top_15_dict:\n",
    "        if key in top_features:\n",
    "            top_features[key] += 1\n",
    "        else:\n",
    "            top_features[key] = 0\n",
    "\n",
    "    print(f\"top features are:{top_15_dict} \\n\")\n",
    "\n",
    "\n",
    "print(f\"top features with combined variance explained: {[key for key, value in top_features.items() if value >= 2.0]}\")\n",
    "\n",
    "\n",
    "# Directly copied from: https://www.geeksforgeeks.org/display-the-pandas-dataframe-in-heatmap-style/\n",
    "df_robust_scaled.corr().style.background_gradient(cmap='viridis') \\\n",
    "    .set_properties(**{'font-size': '20px'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Best Scaling\n",
    "The robust scaling method seems to explain the most variance (aside from no scaling, but the #1 feature there is cid and that's only because it's a large number). But the minmax scaler's top features are more intuitive. Let's save them both and potentially use them both in the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Rename the scaled dataframes\n",
    "df_train_robust = df_robust_scaled\n",
    "df_train_minmax = df_minmax_scaled\n",
    "\n",
    "# Scale the test data with the training data scaling\n",
    "test_robust_scaled = rbst_scaler.transform(df_test)\n",
    "df_test_robust = pd.DataFrame(test_robust_scaled, columns=df_test.columns)\n",
    "test_minmax_scaled = minmax_scaler.transform(df_test)\n",
    "df_test_minmax = pd.DataFrame(test_minmax_scaled, columns=df_test.columns)\n",
    "\n",
    "# Save all the dataframes\n",
    "df_train.to_csv(str(Path(\"data\") / \"train.csv\"), index=False)\n",
    "df_test.to_csv(str(Path(\"data\") / \"test.csv\"), index=False)\n",
    "df_train_robust.to_csv(str(Path(\"data\") / \"train_robust.csv\"), index=False)\n",
    "df_train_minmax.to_csv(str(Path(\"data\") / \"train_minmax.csv\"), index=False)\n",
    "df_test_robust.to_csv(str(Path(\"data\") / \"test_robust.csv\"), index=False)\n",
    "df_test_minmax.to_csv(str(Path(\"data\") / \"test_minmax.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion Observations\n",
    "\n",
    "In essence what we have is a regression problem. We want to predict the price of a house based on the features. Some considerations and lessons learned:\n",
    "1. We have a lot of features, but we've seen with the PCA that reducing it to 3 features we can explain a lot of the variance.\n",
    "2. We've seen that there's some dependence on the location using the coordinates (lng, lat). A Kernel PCA might be useful for that.\n",
    "4. in Addition to 1 We've only removed 1 column, way mre columns can be removed. We could return here and quickly check which columns correlate the least with the price and account for the least variance. But best approach will be to do pca regardless.\n",
    "\n",
    "\n",
    "## Supervised Models to consider\n",
    "1. Multiple Linear Regression\n",
    "2. Above with (kernel) PCA <-- didn't get the kernelPCA to work because of memory issues.\n",
    "3. above with Elastic Net, lasso and/or ridge <-- somewhat redundant with PCA\n",
    "4. More variations of linear regression\n",
    "5. Random Forest (or just decision trees)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
